{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a second notebook due to environment issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lab3_tools as tools\n",
    "import lab3_proto as proto\n",
    "import lab1_proto as proto1\n",
    "import lab1_tools as tools1\n",
    "import lab2_proto as proto2\n",
    "import lab2_tools as tools2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateList = np.load(\"stateList.npy\", allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(stateList)\n",
    "#----\n",
    "trainFile = np.load(\"flatDataTrain.npz\")\n",
    "lmfcc_train_x=trainFile[\"lmfcc_train_x\"].astype('float32')\n",
    "mspec_train_x=trainFile[\"mspec_train_x\"].astype('float32')\n",
    "dlmfcc_train_x=trainFile[\"dlmfcc_train_x\"].astype('float32')\n",
    "dmspec_train_x=trainFile[\"dmspec_train_x\"].astype('float32')\n",
    "train_y=np_utils.to_categorical(trainFile[\"train_y\"],output_dim)\n",
    "#print(lmfcc_train_x.shape,mspec_train_x.shape,dlmfcc_train_x.shape,dmspec_train_x.shape,train_y.shape) #| Shapes checked and OK\n",
    "#----\n",
    "valFile = np.load(\"flatDataVal.npz\")\n",
    "lmfcc_val_x=valFile[\"lmfcc_val_x\"].astype('float32')\n",
    "mspec_val_x=valFile[\"mspec_val_x\"].astype('float32')\n",
    "dlmfcc_val_x=valFile[\"dlmfcc_val_x\"].astype('float32')\n",
    "dmspec_val_x=valFile[\"dmspec_val_x\"].astype('float32')\n",
    "val_y=np_utils.to_categorical(valFile[\"val_y\"],output_dim)\n",
    "#print(lmfcc_val_x.shape,mspec_val_x.shape,dlmfcc_val_x.shape,dmspec_val_x.shape,val_y.shape)\n",
    "#----\n",
    "testFile = np.load(\"flatDataTest.npz\")\n",
    "lmfcc_test_x=testFile[\"lmfcc_test_x\"].astype('float32')\n",
    "mspec_test_x=testFile[\"mspec_test_x\"].astype('float32')\n",
    "dlmfcc_test_x=testFile[\"dlmfcc_test_x\"].astype('float32')\n",
    "dmspec_test_x=testFile[\"dmspec_test_x\"].astype('float32')\n",
    "test_y=np_utils.to_categorical(testFile[\"test_y\"],output_dim)\n",
    "#print(lmfcc_test_x.shape,mspec_test_x.shape,dlmfcc_test_x.shape,dmspec_test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Phoneme Recognition with Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = lmfcc_train_x\n",
    "#labels = train_y\n",
    "\n",
    "features_val_lmfcc = lmfcc_val_x\n",
    "features_val_mspec = mspec_val_x\n",
    "features_val_dlmfcc = dlmfcc_val_x\n",
    "features_val_dmspec = dmspec_val_x\n",
    "labels_val = val_y\n",
    "\n",
    "#feature_dim = features.shape[1]\n",
    "#model = tf.keras.Sequential()\n",
    "#model.add(tf.keras.layers.Dense(80,activation=tf.nn.relu,input_shape=(feature_dim,)))\n",
    "#model.add(tf.keras.layers.Dense(70,activation=tf.nn.relu)) #choose ReLu since it's faster to compute compared to Sigmoid\n",
    "#model.add(tf.keras.layers.Dense(output_dim,activation=tf.nn.softmax)) #softmax distributes probabilitites across our states\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\",\n",
    "            #optimizer=\"sgd\",\n",
    "            #metrics=[\"accuracy\"])\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = 2 #Goes through data set EPOCHS number of times \n",
    "#BATCH_SIZE = 256 #Each training step model will see BATCH_SIZE number of examples to guide and adjust parameters\n",
    "\n",
    "#model.fit(features,labels,epochs=EPOCHS,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 256\n",
    "#print(\"On lmfcc model\")\n",
    "#model_lmfcc = proto.trainDNN(lmfcc_train_x,train_y,EPOCHS,BATCH_SIZE)\n",
    "#print(\"On mspec model\")\n",
    "#model_mspec = proto.trainDNN(mspec_train_x,train_y,EPOCHS,BATCH_SIZE)\n",
    "#print(\"On dlmfcc model\")\n",
    "#model_dlmfcc = proto.trainDNN(dlmfcc_train_x,train_y,EPOCHS,BATCH_SIZE)\n",
    "#print(\"On dmspec model\")\n",
    "#model_dmspec = proto.trainDNN(dmspec_train_x,train_y,EPOCHS,BATCH_SIZE)\n",
    "\n",
    "#model_dmspec.save(\"model_dmspec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of validation data? \n",
    "\n",
    "- Training data: Used to created weights of neural network\n",
    "- Validation data: Used to compare different neural networks and choose the one that gives best result\n",
    "- Test data: Simply used to test performance of the trained and chosen neural network. If not good results, start over. \n",
    "\n",
    "If you use the training and validation data as the same, the choice of neural network can be biased. Which means that the neural network was chosen as better just for that data. Real world data mighjt not have that connection to the neural network and might therefore not perform well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_loss_lmfcc, val_acc_lmfcc = model_lmfcc.evaluate(features_val_lmfcc,labels_val)\n",
    "#print(\"val_acc_lmfcc:\",val_acc_lmfcc) #Can increase with more epochs and more sofisticated model\n",
    "#print(\"val_loss_lmfcc:\",val_loss_lmfcc)\n",
    "#val_loss_mspec, val_acc_mspec = model_mspec.evaluate(features_val_mspec,labels_val)\n",
    "#print(\"val_acc_mspec:\",val_acc_mspec) #Can increase with more epochs and more sofisticated model\n",
    "#print(\"val_loss_mspec:\",val_loss_mspec)\n",
    "#val_loss_dlmfcc, val_acc_dlmfcc = model_dlmfcc.evaluate(features_val_dlmfcc,labels_val)\n",
    "#print(\"val_acc_dlmfcc:\",val_acc_dlmfcc) #Can increase with more epochs and more sofisticated model\n",
    "#print(\"val_loss_dlmfcc:\",val_loss_dlmfcc)\n",
    "#val_loss_dmspec, val_acc_dmspec = model_dmspec.evaluate(features_val_dmspec,labels_val)\n",
    "#print(\"val_acc_dmspec:\",val_acc_dmspec) #Can increase with more epochs and more sofisticated model\n",
    "#print(\"val_loss_dmspec:\",val_loss_dmspec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Detailed Evaluation\n",
    "\n",
    "Looking at the results above, we can see that the best model according to the validation data is the dynamic features. Since the dmspec is slighty better, that one will be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1527014, 61)\n",
      "1.0000000092451684\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#model_dmspec = keras.models.load_model(\"model_dmspec\")\n",
    "#prediction = model_dmspec.predict(dmspec_test_x)\n",
    "#np.save(\"predictionDmspecE2B256.npy\",prediction)\n",
    "prediction = np.load(\"predictionDmspecE2B256.npy\")\n",
    "print(type(prediction))\n",
    "print(prediction.shape) #each row sums to 1\n",
    "print(sum(prediction[8]))\n",
    "print(test_y[456])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(prediction.shape[0]):\n",
    "    row = prediction[i,:]\n",
    "    maxIndex = prediction[i,:].argmax()\n",
    "    prediction[i,:] = np.zeros(row.shape)\n",
    "    prediction[i,maxIndex] = 1\n",
    "\n",
    "print(prediction[456])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Frame-by-frame at state level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global score is: 0.5887667041690515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "global_score = 0\n",
    "max_score = prediction.shape[0]\n",
    "for i in range(max_score):\n",
    "    if (prediction[i] == test_y[i]).all():\n",
    "        global_score += 1\n",
    "\n",
    "print(\"global score is:\", global_score/max_score)\n",
    "\n",
    "#pred_list = proto.convert2indexArray(prediction)\n",
    "#np.save(\"pred_listDmspecE2B256.npy\",pred_list)\n",
    "#print(pred_list)\n",
    "#print(\"on test_y_list\")\n",
    "#test_y_list = proto.convert2indexArray(test_y)\n",
    "#np.save(\"test_y_list.npy\",test_y_list)\n",
    "#conf_matr1 = confusion_matrix(test_y,prediction)\n",
    "#disp1 = ConfusionMatrixDisplay(conf_matr1)\n",
    "#disp1.plot()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = np.load(\"pred_listDmspecE2B256.npy\")\n",
    "test_y_list = np.load(\"test_y_list.npy\")\n",
    "\n",
    "pred_list_states = proto.indexArray2stringArray(pred_list)\n",
    "test_y_list_states = proto.indexArray2stringArray(test_y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Frame-by-frame at phoneme level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sil_1', 'sil_2', 'sil_2', 'sil_1', 'sil_2']\n",
      "['sil', 'sil', 'sil', 'sil', 'sil']\n",
      "['s', 's', 's', 'sp', 'sp']\n"
     ]
    }
   ],
   "source": [
    "phonemes = ['ow','z', 'iy', 'r','w', 'ah', 'n', 'uw','th','f', 'ao','ay', 'v','s', 'ih', 'k','eh','ey', 't','sil','sp']\n",
    "print(pred_list_states[123:128])\n",
    "#pred_list_phonemes = proto.stateArray2phonemeArray(pred_list_states)\n",
    "#test_y_list_phonemes = proto.stateArray2phonemeArray(test_y_list_states)\n",
    "#np.save(\"pred_list_phonemes.npy\",pred_list_phonemes)\n",
    "#np.save(\"test_y_list_phonemes.npy\",test_y_list_phonemes)\n",
    "#print(pred_list_phonemes[123:128])\n",
    "#print(test_y_list_phonemes[123:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251163\n",
      "96749\n",
      "global score is: 0.054997984475291736\n"
     ]
    }
   ],
   "source": [
    "pred_list_phonemes = np.load(\"pred_list_phonemes.npy\")\n",
    "test_y_list_phonemes = np.load(\"test_y_list_phonemes.npy\")\n",
    "\n",
    "pred_list_phonemes = proto.mergePhonemes(pred_list_phonemes)\n",
    "test_y_list_phonemes = proto.mergePhonemes(test_y_list_phonemes)\n",
    "\n",
    "print(len(pred_list_phonemes))\n",
    "print(len(test_y_list_phonemes))\n",
    "\n",
    "global_score = 0\n",
    "max_score = len(test_y_list_phonemes)\n",
    "for i,phoneme in enumerate(test_y_list_phonemes): # OBS! different lengths, choose shortest\n",
    "    if phoneme == pred_list_phonemes[i]:\n",
    "        global_score += 1\n",
    "\n",
    "print(\"global score is:\", global_score/max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Edit distance at the state level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_statesMerged = proto.mergePhonemes(pred_list_states)\n",
    "test_y_list_statesMerged = proto.mergePhonemes(test_y_list_states)\n",
    "print(pred_list_statesMerged[456:460])\n",
    "print(test_y_list_statesMerged[456:460])\n",
    "\n",
    "PER = proto.levenshteinDistance(pred_list_statesMerged,test_y_list_statesMerged)\n",
    "print(PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Edit distance at the phoneme level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Possible questions\n",
    "\n",
    "- what is the influence of feature kind and size of input context window?\n",
    "    - The input kind is important since that reveals the amount of \"hidden data\". Ex. the lmfcc has been stripped of some information compared to the mspecs. This can later be interpred by the hidden layers. \n",
    "    -   input context window $\\color{red}{!!!}$\n",
    "- what is the purpose of normalising (standardising) the input feature vectors depending on\n",
    "the activation functions in the network?\n",
    "    - Because otherwise some values that shouldn't activate the activation function will do so and vice versa. When normalising all values, one ensures that all data is represented in relation to each other.\n",
    "    - Looking at what happens when one only normalises for each utterance, some parts of an utterance might trigger the activation function even though it wouldn't have if one normalised all utterances in relation to each other. This is why shorter utterances might be over represented when one only normalises for each utternace in isolation.\n",
    "- what is the influence of the number of units per layer and the number of layers?\n",
    "    - Number of units can be translated into resolution of hidden features.\n",
    "    - Number of layers can ber translated into the different component division of the features. Ex. image of numbers, divide a 9 into a line and loop etc.\n",
    "- what is the influence of the activation function (when you try other activation functions\n",
    "than ReLU, you do not need to reach convergence in case you do not have enough time)\n",
    "    - Computational convergence time.\n",
    "- what is the influence of the learning rate/learning rate strategy?\n",
    "    - You want a DNN that increases it's accuracy a lot for each epoch\n",
    "- how stable are the posteriograms from the network in time?\n",
    "    - answer\n",
    "- how do the errors distribute depending on phonetic class?\n",
    "    - answer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c56dde32312b51f3125b3da835da250fcf310d1475bfad666419fd8986aabd3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
