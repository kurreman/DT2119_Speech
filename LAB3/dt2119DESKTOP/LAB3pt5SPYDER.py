import numpy as npimport lab3_tools as toolsimport lab3_proto as protoimport lab1_proto as proto1import lab1_tools as tools1import lab2_proto as proto2import lab2_tools as tools2import matplotlib.pyplot as pltimport osfrom sklearn.preprocessing import StandardScalerimport tensorflow as tffrom tensorflow import keras#from tensorflow.keras.utils import np_utilsfrom keras.utils import np_utilsstateList = np.load("stateList.npy", allow_pickle=True).tolist()output_dim = len(stateList)#----trainFile = np.load("flatDataTrain.npz")lmfcc_train_x=trainFile["lmfcc_train_x"].astype('float32')mspec_train_x=trainFile["mspec_train_x"].astype('float32')dlmfcc_train_x=trainFile["dlmfcc_train_x"].astype('float32')dmspec_train_x=trainFile["dmspec_train_x"].astype('float32')train_y=np_utils.to_categorical(trainFile["train_y"],output_dim)#print(lmfcc_train_x.shape,mspec_train_x.shape,dlmfcc_train_x.shape,dmspec_train_x.shape,train_y.shape) #| Shapes checked and OK#----valFile = np.load("flatDataVal.npz")lmfcc_val_x=valFile["lmfcc_val_x"].astype('float32')mspec_val_x=valFile["mspec_val_x"].astype('float32')dlmfcc_val_x=valFile["dlmfcc_val_x"].astype('float32')dmspec_val_x=valFile["dmspec_val_x"].astype('float32')val_y=np_utils.to_categorical(valFile["val_y"],output_dim)#print(lmfcc_val_x.shape,mspec_val_x.shape,dlmfcc_val_x.shape,dmspec_val_x.shape,val_y.shape)#----testFile = np.load("flatDataTest.npz")lmfcc_test_x=testFile["lmfcc_test_x"].astype('float32')mspec_test_x=testFile["mspec_test_x"].astype('float32')dlmfcc_test_x=testFile["dlmfcc_test_x"].astype('float32')dmspec_test_x=testFile["dmspec_test_x"].astype('float32')test_y=np_utils.to_categorical(testFile["test_y"],output_dim)#print(lmfcc_test_x.shape,mspec_test_x.shape,dlmfcc_test_x.shape,dmspec_test_x.shape,test_y.shape)features = lmfcc_train_xlabels = train_yfeatures_test = lmfcc_test_xlabels_test = test_yfeature_dim = features.shape[1]model = tf.keras.Sequential()model.add(tf.keras.layers.Dense(80,activation=tf.nn.relu,input_shape=(feature_dim,)))model.add(tf.keras.layers.Dense(output_dim,activation=tf.nn.softmax)) #softmax distributes probabilitites across our statesmodel.compile(loss="categorical_crossentropy",            optimizer="sgd",            metrics=["accuracy"])model.summary()EPOCHS = 2 #Goes through data set EPOCHS number of times BATCH_SIZE = 256 #Each training step model will see BATCH_SIZE number of examples to guide and adjust parametersmodel.fit(features,labels,epochs=EPOCHS,batch_size=BATCH_SIZE)test_loss, test_acc = model.evaluate(features_test,labels_test)print("test_acc:",test_acc) #Can increase with more epochs and more sofisticated modelprint("test_loss:",test_loss)